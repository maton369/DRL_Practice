"""
overview:
    価値反復法（Value Iteration）により 3状態×2行動 のマルコフ決定過程（MDP）を解く。

    価値反復法は「最適ベルマン作用素」によって価値関数を直接更新するアルゴリズムであり、
    方策反復法（Policy Iteration）と違って “方策評価→方策改善” を分けず、
    次の更新式（ベルマン最適方程式の反復）を繰り返す。

    V_{k+1}(s) = max_a Q_k(s,a)
    Q_k(s,a)  = E[ r(s,a,s') + γ V_k(s') ]

args:
    各種パラメータ設定値は、本コード中に明記される

output:
    反復ステップごとに 3状態の価値関数値 V(s) および方策確率 π(s)=P(a0|s) を出力する

usage-example:
    python3 value_iteraton.py
"""

# =========================
# 依存ライブラリ
# =========================
import numpy as np
import copy

# =========================
# MDP の定義（このコードでの表現）
# =========================
# 状態数: 3（状態 s = 0,1,2）
# 行動数: 2（行動 a0, a1）
#
# この実装では「方策 π」は各状態 i について
#   pi[i] = P(a0 | s=i)
# として表現している（つまり a1 の確率は 1 - pi[i]）。
#
# 遷移モデルは「状態 i で行動 a0 を取ったときのみ確率分岐」し、
# 行動 a1 のときは “必ず状態が変わらない（self-loop）” と固定している。
#
# 具体的には:
#   - 行動 a0 を選んだとき:
#       確率 p[i]   で (i+1)%3 へ遷移
#       確率 1-p[i] で (i+2)%3 へ遷移
#   - 行動 a1 を選んだとき:
#       確率 1 で i へ遷移（self-loop）

# -------------------------
# 遷移確率パラメータ p[i]
# -------------------------
p = [0.8, 0.5, 1.0]

# -------------------------
# 割引率 γ
# -------------------------
# 未来の報酬をどれだけ重視するかを決めるパラメータ。
# 0<γ<1 の範囲だと、無限和が収束し、価値反復も収束しやすい（縮小写像）。
gamma = 0.95

# -------------------------
# 報酬テーブル r[s, s', a]
# -------------------------
# 「状態 s で行動 a を選び、次状態 s' に遷移したときの即時報酬」を格納する。
# shape=(3,3,2)
#   第1軸: 現状態 s
#   第2軸: 次状態 s'
#   第3軸: 行動 a（0 or 1）
r = np.zeros((3, 3, 2))

# 状態0
r[0, 1, 0] = 1.0  # s=0, a0 で s'=1 なら +1
r[0, 2, 0] = 2.0  # s=0, a0 で s'=2 なら +2
r[0, 0, 1] = 0.0  # s=0, a1（self-loop）なら 0

# 状態1
r[1, 0, 0] = 1.0
r[1, 2, 0] = 2.0
r[1, 1, 1] = 1.0  # s=1, a1（self-loop）なら +1

# 状態2
r[2, 0, 0] = 1.0
r[2, 1, 0] = 0.0
r[2, 2, 1] = -1.0  # s=2, a1（self-loop）なら -1

# =========================
# 変数の初期化
# =========================

# 価値関数 V(s) の初期値（長さ3）
# 価値反復法では、初期値から最適価値関数 V* に向かって更新していく。
# 初期値は任意だが、収束速度や途中の表示に影響は出る。
v = [0, 0, 0]

# v_new は「次の反復で計算された価値関数」を入れるための変数。
# 価値反復では v と v_new を分けておくと更新の意味が明確になる。
v_new = copy.copy(v)

# 行動価値関数 Q(s,a) を格納する領域（shape=(3,2)）
# 各反復で上書きされる。
q = np.zeros((3, 2))

# 方策 π の初期値
# ここでは “出力のために” 方策（greedy）を毎ステップ計算している。
# ※価値反復の本質は V の更新であり、π は最後に greedy で復元してもよい。
pi = [0.5, 0.5, 0.5]

# =========================
# 価値反復（Value Iteration）
# =========================
# 最大 1000 ステップまで反復（小さいMDPならもっと早く収束する想定）
for step in range(1000):

    # ------------------------------------------------------------
    # 各状態 i で Q(i,0), Q(i,1) を計算する
    # ------------------------------------------------------------
    # 価値反復の更新式は以下：
    #
    #   Q_k(s,a)  = E[ r(s,a,s') + γ V_k(s') ]
    #   V_{k+1}(s)= max_a Q_k(s,a)
    #
    # ここではまず Q を全状態分計算し、そのあと max を取って v_new を作る。
    for i in range(3):

        # -------------------------
        # 行動 a0 の行動価値 Q(i,0)
        # -------------------------
        # a0 のときは確率分岐がある。
        #
        #   Q(i,0) = p[i]   * ( r(i, i+1, 0) + γ V(i+1) )
        #         + (1-p[i])* ( r(i, i+2, 0) + γ V(i+2) )
        #
        # ※ここでの V は「前ステップの価値関数 v = V_k」を参照している点が重要。
        q[i, 0] = p[i] * (r[i, (i + 1) % 3, 0] + gamma * v[(i + 1) % 3]) + (
            1 - p[i]
        ) * (r[i, (i + 2) % 3, 0] + gamma * v[(i + 2) % 3])

        # -------------------------
        # 行動 a1 の行動価値 Q(i,1)
        # -------------------------
        # a1 のときは self-loop 固定なので
        #
        #   Q(i,1) = r(i,i,1) + γ V(i)
        q[i, 1] = r[i, i, 1] + gamma * v[i]

        # --------------------------------------------------------
        # （出力用）greedy 方策の計算
        # --------------------------------------------------------
        # 本質的には v_new を作るときに argmax を取れば方策は復元できるが、
        # 毎ステップの方策を見たいので、ここで greedy を計算している。
        #
        # pi[i] = P(a0|i)
        #   - Q(i,0) > Q(i,1) なら a0 を確率1で選ぶ => pi[i]=1
        #   - Q(i,0) < Q(i,1) なら a0 を確率0で選ぶ => pi[i]=0
        #   - 同値なら混合 => pi[i]=0.5
        #
        # 注意: 浮動小数点誤差があるため、== 比較は “完全一致” になりにくいことがある。
        if q[i, 0] > q[i, 1]:
            pi[i] = 1
        elif q[i, 0] == q[i, 1]:
            pi[i] = 0.5
        else:
            pi[i] = 0

    # ------------------------------------------------------------
    # ベルマン最適作用素による価値更新
    # ------------------------------------------------------------
    # 状態ごとに「より良い行動（価値が最大の行動）」を選ぶことで
    # 次の価値関数を更新する。
    #
    #   v_new[s] = max_a q[s,a]
    #
    # ここで np.max(q, axis=-1) は shape=(3,) の配列を返す。
    v_new = np.max(q, axis=-1)

    # ------------------------------------------------------------
    # 収束（終了）判定
    # ------------------------------------------------------------
    # 現在の条件:
    #   if min_i (v_new[i] - v[i]) <= 0: break
    #
    # これは「どこかの状態で改善しない（同じ or 下がる）なら停止」という条件であり、
    # 価値反復の標準的な停止条件（変化量が十分小さい）とは少し違う。
    #
    # 価値反復の典型的な停止判定は例えば:
    #
    #   max_i |v_new[i] - v[i]| < ε
    #
    # のように “絶対変化量” で見る。
    #
    # 本コードの条件は「単調増加することを前提にした停止」になっているため、
    # - 初期値の取り方
    # - 負の報酬がある場合
    # - 数値誤差で微小に下がる場合
    # などで早期停止の可能性がある点は注意。
    if np.min(v_new - v) <= 0:
        break

    # 価値関数を更新（次の反復で v が V_k として使われる）
    v = copy.copy(v_new)

    # 現ステップの価値関数と方策を表示
    print("step:", step, " value:", v, " policy:", pi)
