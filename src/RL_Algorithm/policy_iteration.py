"""
overview:
    方策反復法（Policy Iteration）により、3状態×2行動の小さなマルコフ決定過程（MDP）を解く。
    - 方策評価（Policy Evaluation）: 現在の方策 π に対して価値関数 V^π を求める
    - 方策改善（Policy Improvement）: V^π を使って貪欲（greedy）に方策を更新する
    を反復し、最適方策 π*（に相当する決定論的方策）へ近づける。

args:
    各種パラメータ設定値は、本コード中に明記される

output:
    反復ステップごとに 3状態の価値関数値 V(s) および方策確率 π(s)=P(a0|s) を出力する

usage-example:
    python3 policy_iteraton.py
"""

# =========================
# 依存ライブラリ
# =========================
import numpy as np
import copy

# =========================
# MDP の定義（このコードでの表現）
# =========================
# 状態数: 3（状態 s = 0,1,2）
# 行動数: 2（行動 a0, a1）
#
# この実装では「方策 π」は各状態 i について
#   pi[i] = P(a0 | s=i)
# として表現している（つまり a1 の確率は 1 - pi[i]）。
#
# 遷移モデルは状態 i と行動 a0 のとき確率 p[i] で (i+1)%3 に、
# 確率 (1-p[i]) で (i+2)%3 に遷移する、という構造を持つ。
# 行動 a1 のときは「状態が変わらない」としている（self-loop）。
#
# 注意: 「行動 a1 の遷移が必ず self-loop」という前提は、
#       policy_estimator 内で P[i,i] = 1 - pi[i] と固定していることから分かる。

# -------------------------
# 遷移確率パラメータ p[i]
# -------------------------
# 状態 i で行動 a0 を選んだとき
#   次状態 = (i+1)%3 となる確率が p[i]
#   次状態 = (i+2)%3 となる確率が 1-p[i]
p = [0.8, 0.5, 1.0]

# -------------------------
# 割引率 γ
# -------------------------
# 未来報酬の現在価値の重み。0<gamma<1 の範囲。
gamma = 0.95

# -------------------------
# 報酬期待値（報酬テーブル） r[s, s', a]
# -------------------------
# 「状態 s で行動 a を選び、次状態 s' に遷移したときの即時報酬」
# を格納している。
#
# r は shape=(3,3,2)
#   第1軸: 現状態 s
#   第2軸: 次状態 s'
#   第3軸: 行動 a（0 or 1）
r = np.zeros((3, 3, 2))

# 状態0
r[0, 1, 0] = 1.0  # s=0 で a0 を選び、s'=1 に行ったら報酬1
r[0, 2, 0] = 2.0  # s=0 で a0 を選び、s'=2 に行ったら報酬2
r[0, 0, 1] = 0.0  # s=0 で a1 を選び、s'=0（self-loop）なら報酬0

# 状態1
r[1, 0, 0] = 1.0
r[1, 2, 0] = 2.0
r[1, 1, 1] = 1.0  # s=1 で a1（self-loop）報酬1

# 状態2
r[2, 0, 0] = 1.0
r[2, 1, 0] = 0.0
r[2, 2, 1] = -1.0  # s=2 で a1（self-loop）報酬-1

# =========================
# 変数の初期化
# =========================

# 価値関数 V(s) の初期値（長さ3）
# 方策反復では、評価ステップで厳密に解くなら初期値は本質的に不要だが、
# ここでは「改善が止まったら終了」の判定に v_prev を使うため持っている。
v = [0, 0, 0]
v_prev = copy.copy(v)  # 前ステップの価値関数を保持（deep copy の簡易版）

# 行動価値関数 Q(s,a) の格納領域（shape=(3,2)）
# 方策改善ステップで毎回上書きする。
q = np.zeros((3, 2))

# 方策 π の初期値
# pi[i] = P(a0|s=i)
# すべての状態で a0/a1 を半々に選ぶ初期方策になっている。
pi = [0.5, 0.5, 0.5]


# =========================
# 方策評価（Policy Evaluation）
# =========================
def policy_estimator(pi, p, r, gamma):
    """
    与えられた方策 π に対して、価値関数 V^π を「行列で厳密に」求める関数。

    方策評価のベルマン方程式は次で与えられる:
        V = R^π + γ P^π V
    ここで
        P^π: 方策πのもとでの状態遷移確率行列（3x3）
        R^π: 方策πのもとでの期待即時報酬ベクトル（長さ3）
    したがって
        (I - γ P^π) V = R^π
        V = (I - γ P^π)^{-1} R^π

    この関数では、P^π と R^π を構築して上式を解く。
    """

    # R: 報酬ベクトル（各状態の期待即時報酬）を格納
    R = [0, 0, 0]

    # P: 遷移行列（方策πのもとでの P^π）を格納
    P = np.zeros((3, 3))

    # A: (I - γP) を格納する行列（後で上書きする）
    A = np.zeros((3, 3))

    for i in range(3):
        # -------------------------
        # 状態遷移行列 P^π の計算
        # -------------------------
        # このコードのMDP仕様:
        #   - 行動 a0 を選ぶ確率 = pi[i]
        #     そのとき
        #       s'=(i+1)%3 へ確率 p[i]
        #       s'=(i+2)%3 へ確率 1-p[i]
        #   - 行動 a1 を選ぶ確率 = 1-pi[i]
        #     そのときは self-loop（s'=i）へ確率1
        #
        # よって、方策πでの遷移確率は
        #   P[i,i]          = P(a1|i)*1 = 1 - pi[i]
        #   P[i,(i+1)%3]    = P(a0|i)*p[i] = pi[i]*p[i]
        #   P[i,(i+2)%3]    = P(a0|i)*(1-p[i]) = pi[i]*(1-p[i])

        P[i, i] = 1 - pi[i]
        P[i, (i + 1) % 3] = p[i] * pi[i]
        P[i, (i + 2) % 3] = (1 - p[i]) * pi[i]

        # -------------------------
        # 報酬ベクトル R^π の計算
        # -------------------------
        # R[i] = E[ r(s=i, a~π, s') ]
        #
        # 行動 a0 を取った場合は確率分岐があるので期待値:
        #   E[r | a0, i] = p[i]*r[i,(i+1)%3,0] + (1-p[i])*r[i,(i+2)%3,0]
        #
        # 行動 a1 を取った場合は self-loop 固定なので:
        #   r[i,i,1]
        #
        # 以上を方策確率 pi[i] で混ぜる:
        #   R[i] = pi[i]*E[r|a0,i] + (1-pi[i])*r[i,i,1]

        R[i] = (
            pi[i] * (p[i] * r[i, (i + 1) % 3, 0] + (1 - p[i]) * r[i, (i + 2) % 3, 0])
            + (1 - pi[i]) * r[i, i, 1]
        )

    # -------------------------
    # 行列計算によるベルマン方程式の求解
    # -------------------------
    # A = I - γP,  V = A^{-1}R
    #
    # 注意:
    # - 状態数が大きい場合、逆行列を明示的に計算するのは数値的にも計算量的にも不利。
    #   その場合は np.linalg.solve(A, R) を使うのが一般的。
    # - ここは 3x3 と小さいので逆行列でも問題になりにくい。

    A = np.eye(3) - gamma * P
    B = np.linalg.inv(A)
    v_sol = np.dot(B, R)

    return v_sol


# =========================
# 方策反復（Policy Iteration）
# =========================
# 最大100ステップまで反復（小さなMDPならすぐ収束する想定）
for step in range(100):

    # -------------------------
    # 方策評価ステップ
    # -------------------------
    # 現在の方策 pi に対応する価値関数 V^π を厳密に計算する
    v = policy_estimator(pi, p, r, gamma)

    # -------------------------
    # 収束（終了）判定
    # -------------------------
    # このコードでは「前回より改善しなくなったら終了」としている:
    #   if min_i (v[i] - v_prev[i]) <= 0: break
    #
    # つまり、どれか1状態でも価値が同じか下がれば打ち切る判定であり、
    # “標準的な方策反復の収束判定” とは少し違う。
    #
    # 標準的には
    #   - 方策が変化しなくなったら終了（π_{k+1} == π_k）
    #   - あるいは価値関数の変化量が十分小さくなったら終了（||V_{k+1}-V_k|| < ε）
    # のような基準を使うことが多い。
    #
    # ただしこの例は状態数が小さく、方策が決定論（0 or 1）に寄っていくので、
    # 結果的に早く止まることが多い。

    if np.min(v - v_prev) <= 0:
        break

    # 現ステップの価値関数と方策を表示
    # v は numpy 配列、pi は各状態の a0 の確率
    print("step:", step, " value:", v, " policy:", pi)

    # -------------------------
    # 方策改善ステップ
    # -------------------------
    # 各状態 i について、Q(i,a0), Q(i,a1) を計算し、
    # それが大きい行動を確率1で選ぶ（greedy）ように方策を更新する。
    #
    # ここでの Q は「1ステップ先 + その先は V で近似」という、
    # ベルマン最適性に基づく典型形:
    #   Q(s,a) = E[ r(s,a,s') + γ V(s') ]
    #
    # なお、この改善は “V が現在の方策 π の価値” であるため、
    # 方策改善定理により π は悪化しない（理論上は改善 or 不変）。
    for i in range(3):

        # -------------------------
        # 行動 a0 の行動価値 Q(i,0)
        # -------------------------
        # a0 のときは確率 p[i] で (i+1)%3 に、確率 1-p[i] で (i+2)%3 に遷移する。
        #
        # よって期待値は
        #   Q(i,0) = p[i]*( r(i,(i+1),0) + γ V(i+1) )
        #         + (1-p[i])*( r(i,(i+2),0) + γ V(i+2) )

        q[i, 0] = p[i] * (r[i, (i + 1) % 3, 0] + gamma * v[(i + 1) % 3]) + (
            1 - p[i]
        ) * (r[i, (i + 2) % 3, 0] + gamma * v[(i + 2) % 3])

        # -------------------------
        # 行動 a1 の行動価値 Q(i,1)
        # -------------------------
        # a1 のときは self-loop なので次状態は i 固定。
        #   Q(i,1) = r(i,i,1) + γ V(i)
        q[i, 1] = r[i, i, 1] + gamma * v[i]

        # -------------------------
        # greedy に方策更新
        # -------------------------
        # pi[i] = P(a0|i)
        #   - Q(i,0) > Q(i,1) なら a0 を確率1で選ぶ => pi[i]=1
        #   - Q(i,0) < Q(i,1) なら a0 を確率0で選ぶ => pi[i]=0（a1 を確率1）
        #   - 同値なら混合も許す => pi[i]=0.5
        #
        # この “同値なら0.5” は「最適方策が複数あるときに混合を残す」ための処理。
        # 実務では浮動小数点誤差があるので、== 比較は注意が必要な場合がある。
        if q[i, 0] > q[i, 1]:
            pi[i] = 1
        elif q[i, 0] == q[i, 1]:
            pi[i] = 0.5
        else:
            pi[i] = 0

    # -------------------------
    # 現ステップの価値関数を記録
    # -------------------------
    # 次の反復で「改善したかどうか」を判定するために保持しておく
    v_prev = copy.copy(v)
